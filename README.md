# problem-difficulty
ğŸ“Œ Project OverviewAutoJudge is an intelligent machine learning system designed to automatically assess the complexity of programming problems. By analyzing the textual description of a problem (including input/output details), the system predicts:Difficulty Class: Classifies the problem as Easy, Medium, or Hard.Complexity Score: Predicts a precise numerical difficulty score.This project was built to automate the categorization process often found on coding platforms like Codeforces or LeetCode, removing the need for manual tagging.ğŸ“‚ DatasetThe project uses the TaskComplexity dataset (provided as problems_data.jsonl).Source: Web-scraped programming problems.Size: ~4,000 tasks.Features Used: title, description, input_description, output_description.Target Variables:problem_class (Categorical: Easy, Medium, Hard)problem_score (Numerical)ğŸ› ï¸ Methodology & Approach1. Data PreprocessingRaw text from programming problems is noisy. We applied the following cleaning steps:Text Combination: Merged Title, Description, Input, and Output fields into a single context string.LaTeX Removal: Stripped mathematical symbols (e.g., $N \le 10^5$) to focus on linguistic complexity.Normalization: Converted text to lowercase and removed special characters/excess whitespace.2. Feature EngineeringWe used TF-IDF (Term Frequency-Inverse Document Frequency) to convert text into numerical vectors.Why? It highlights unique words that differentiate difficulty levels (e.g., "array" might appear in Easy problems, while "dynamic", "graph", or "optimization" appear in Hard ones).Settings: We limited the vectorizer to the top 3,000â€“5,000 features to prevent overfitting.3. Model Selection & TuningWe did not just pick one model; we ran a Grid Search competition to find the best one.Classification Task (Easy/Medium/Hard)We compared the following models:Logistic Regression: A strong baseline for text classification.Random Forest Classifier: Handles non-linear relationships well.Support Vector Machine (SVM): Effective in high-dimensional text spaces.Regression Task (Numerical Score)We compared the following models:Linear Regression (Ridge): Linear modeling with regularization.Random Forest Regressor: Ensemble averaging for robust scoring.Gradient Boosting: Sequential error correction for high precision.The system automatically saves the model with the highest Accuracy (Classification) and lowest Mean Absolute Error (Regression).ğŸ“Š ResultsAfter training and hyperparameter tuning, the best performing models were selected based on the test set (20% split).(Note: Replace the values below with the actual numbers from your train_and_tune.py output)TaskBest Model SelectedPerformance MetricClassificatione.g., Support Vector Machine (SVM)Accuracy: ~XX%Regressione.g., Gradient BoostingMAE: ~X.XXğŸš€ How to Run the ProjectPrerequisitesInstall the required libraries:Bashpip install -r requirements.txt
Step 1: Train & Tune ModelsRun the training script. This will load the data, compare all models, and save the winners as .pkl files.Bashpython train_and_tune.py
Output: You will see a log of models being tested and the "Winner" for each task.Step 2: Launch the Web UIStart the Streamlit interface to test the models interactively.Bashstreamlit run app.py
This will open a browser window where you can paste any problem description to see the AI's prediction.ğŸ“ Project StructurePlaintextâ”œâ”€â”€ problems_data.jsonl    # Source Dataset
â”œâ”€â”€ train_and_tune.py      # Main script for Training, Tuning & Evaluation
â”œâ”€â”€ app.py                 # Streamlit Web Interface
â”œâ”€â”€ requirements.txt       # List of dependencies
â”œâ”€â”€ README.md              # Project Documentation
â”œâ”€â”€ best_classifier.pkl    # Saved Best Classification Model (Generated)
â””â”€â”€ best_regressor.pkl     # Saved Best Regression Model (Generated)
